{
 "metadata": {
  "name": "",
  "signature": "sha256:418b0f6279b8c7a24a330adf2c2d87463182c4727b1db3cc6b84c52d34e5d9b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "N = 100 #number of points per class\n",
      "D = 2 #dimensionality\n",
      "K = 3 # number of classes\n",
      "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
      "y = np.zeros(N*K, dtype = 'uint8')# class labels\n",
      "for j in xrange(K):\n",
      "    ix = range(N*j, N*(j+1))\n",
      "    r = np.linspace(0.0, 1, N) #radius\n",
      "    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N) * 0.2#theta\n",
      "    X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n",
      "    y[ix] = j\n",
      "#lets visualize the data:\n",
      "plt.scatter(X[:, 0], X[:, 1], c = y, s = 40, cmap = plt.cm.Spectral)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Train a Linear Classifier\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#initialize parameters randomly\n",
      "W = 0.01 * np.random.randn(D, K)\n",
      "b = np.zeros((1, K))\n",
      "\n",
      "#some hyperparameters\n",
      "step_size = 1e-0\n",
      "reg = 1e-3 # regularization strength\n",
      "#gradient descent loop\n",
      "num_examples = X.shape[0]\n",
      "for i in xrange(200):\n",
      "    #evaluate class scores,[N x K]\n",
      "    scores = np.dot(X, W) + b\n",
      "    #compute the class probabilities\n",
      "    exp_scores = np.exp(scores)\n",
      "    probs = exp_scores / np.sum(exp_scores, axis = 1, keepdims = True) #[N x K]\n",
      "    #compute the loss: average cross-entropy loss and regularization\n",
      "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
      "    data_loss = np.sum(corect_logprobs) / num_examples\n",
      "    reg_loss = 0.5 * reg * np.sum(W * W)\n",
      "    loss = data_loss + reg_loss\n",
      "    if i % 10 == 0:\n",
      "        print \"iteration %d: loss %f\" %(i, loss)\n",
      "    #compute the gradient on scores\n",
      "    dscores = probs\n",
      "    dscores[range(num_examples), y] -= 1\n",
      "    dscores /= num_examples\n",
      "    #backpropate the gradient to the parameters (W,b)\n",
      "    dW = np.dot(X.T, dscores)\n",
      "    db = np.sum(dscores, axis = 0, keepdims = True)\n",
      "\n",
      "    dW += reg * W # regularization gradient\n",
      "    #perform a parameter update\n",
      "    W += -step_size * dW\n",
      "    b += -step_size * db\n",
      "\n",
      "#evaluate training set accuracy\n",
      "scores = np.dot(X, W)+b\n",
      "predicted_class = np.argmax(scores, axis = 1)\n",
      "print 'training accuracy: %.2f' % (np.mean(predicted_class == y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration 0: loss 1.097734\n",
        "iteration 10: loss 0.908367\n",
        "iteration 20: loss 0.838834\n",
        "iteration 30: loss 0.807799\n",
        "iteration 40: loss 0.791955\n",
        "iteration 50: loss 0.783117\n",
        "iteration 60: loss 0.777876\n",
        "iteration 70: loss 0.774630\n",
        "iteration 80: loss 0.772553\n",
        "iteration 90: loss 0.771190\n",
        "iteration 100: loss 0.770278\n",
        "iteration 110: loss 0.769659\n",
        "iteration 120: loss 0.769232\n",
        "iteration 130: loss 0.768936\n",
        "iteration 140: loss 0.768729\n",
        "iteration 150: loss 0.768582\n",
        "iteration 160: loss 0.768479\n",
        "iteration 170: loss 0.768404\n",
        "iteration 180: loss 0.768351\n",
        "iteration 190: loss 0.768313\n",
        "training accuracy: 0.52\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot the resulting classifier\n",
      "h = 0.02\n",
      "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
      "                     np.arange(y_min, y_max, h))\n",
      "Z = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
      "Z = np.argmax(Z, axis=1)\n",
      "Z = Z.reshape(xx.shape)\n",
      "fig = plt.figure()\n",
      "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
      "plt.xlim(xx.min(), xx.max())\n",
      "plt.ylim(yy.min(), yy.max())\n",
      "plt.show()\n",
      "#fig.savefig('spiral_linear.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Train a Neural Network\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "#initialize parameters randomly\n",
      "h = 100 # size of hidden layer\n",
      "W = 0.01 * np.random.randn(D, h)\n",
      "b = np.zeros((1, h))\n",
      "W2 = 0.01 * np.random.randn(h, K)\n",
      "b2 = np.zeros((1, K))\n",
      "#some hyperparameters\n",
      "step_size = 1e-0\n",
      "reg = 1e-3 # regularization strength\n",
      "#gradient descent loop\n",
      "num_examples = X.shape[0]\n",
      "for i in xrange(10000):\n",
      "    #evaluate class scores, [N x K]\n",
      "    hidden_layer = np.maximum(0, np.dot(X, W) + b)# note, ReLU activation\n",
      "    scores = np.dot(hidden_layer, W2) + b2\n",
      "    #compute the class probabilities\n",
      "    exp_scores = np.exp(scores)\n",
      "    probs = exp_scores / np.sum(exp_scores, axis = 1, keepdims = True)#[N x K]\n",
      "    #compute the loss: average cross-entropy loss and regularization\n",
      "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
      "    data_loss = np.sum(corect_logprobs) / num_examples\n",
      "    reg_loss = 0.5 * reg * np.sum(W * W) + 0.5 * reg * np.sum(W2 * W2)\n",
      "    loss = data_loss + reg_loss\n",
      "    if i % 1000 == 0:\n",
      "        print \"iteration %d: loss %f\" %(i, loss)\n",
      "    \n",
      "    #compute the gradient on scores\n",
      "    dscores = probs\n",
      "    dscores[range(num_examples), y] -= 1\n",
      "    dscores /= num_examples\n",
      "    \n",
      "    #backpropate the gradient to the parameters\n",
      "    #first backprop into parameters W2 and b2\n",
      "    dW2 = np.dot(hidden_layer.T, dscores)\n",
      "    db2 = np.sum(dscores, axis = 0, keepdims = True)\n",
      "    #next backprop into hidden layer\n",
      "    dhidden = np.dot(dscores, W2.T)\n",
      "    #backprop the ReLU non-linearity\n",
      "    dhidden[hidden_layer <= 0] = 0\n",
      "    #finally into W, b\n",
      "    dW = np.dot(X.T, dhidden)\n",
      "    db = np.sum(dhidden, axis = 0, keepdims = True)\n",
      "    \n",
      "    #add regularization gradient contribution\n",
      "    dW2 += reg * W2\n",
      "    dW += reg * W\n",
      "    \n",
      "    #perform a parameter update\n",
      "    W += -step_size * dW\n",
      "    b += -step_size * db\n",
      "    W2 += -step_size * dW2\n",
      "    b2 += -step_size * db2\n",
      "    \n",
      "#evaluate training set accuracy\n",
      "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
      "scores = np.dot(hidden_layer, W2) + b2\n",
      "predicted_class = np.argmax(scores, axis = 1)\n",
      "print 'training accuracy: %.2f' % (np.mean(predicted_class == y))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration 0: loss 1.098690\n",
        "iteration 1000: loss 0.314539"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 2000: loss 0.262702"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 3000: loss 0.258061"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 4000: loss 0.254015"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 5000: loss 0.252719"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 6000: loss 0.252137"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 7000: loss 0.251690"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 8000: loss 0.251493"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "iteration 9000: loss 0.251384"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "training accuracy: 0.98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot the resulting classifier\n",
      "h = 0.02\n",
      "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
      "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
      "                     np.arange(y_min, y_max, h))\n",
      "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b), W2) + b2\n",
      "Z = np.argmax(Z, axis=1)\n",
      "Z = Z.reshape(xx.shape)\n",
      "fig = plt.figure()\n",
      "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
      "plt.xlim(xx.min(), xx.max())\n",
      "plt.ylim(yy.min(), yy.max())\n",
      "plt.show()\n",
      "#fig.savefig('spiral_net.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}